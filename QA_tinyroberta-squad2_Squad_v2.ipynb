{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aaa4edb8",
   "metadata": {},
   "source": [
    "# Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1315a7fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-13 22:09:17.026025: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import json\n",
    "import sys\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "import collections\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "import datasets\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ed08d27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version:  1.12.1\n",
      "transformer version:  4.24.0\n",
      "dataset version:  2.10.1\n"
     ]
    }
   ],
   "source": [
    "print(\"torch version: \", torch.__version__)\n",
    "print(\"transformer version: \", transformers.__version__)\n",
    "print(\"dataset version: \", datasets.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b89120b",
   "metadata": {},
   "source": [
    "# Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ccfaabd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_random(seed: int) -> None:\n",
    "    \"\"\"Fix all the possible sources of randomness.\n",
    "\n",
    "    Args:\n",
    "        seed: the seed to use.\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "438607b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_random(seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410f0077",
   "metadata": {},
   "source": [
    "# Runtime Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74d9d2fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Device:  cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using Device: \", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e3b2d1",
   "metadata": {},
   "source": [
    "# Load SQuAD v2 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b41b135c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset squad_v2 (/Users/Home/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bc1ea182981445ca0bb28d31db35a56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = load_dataset(\"squad_v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7c85cb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 130319\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 11873\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "38ed6090",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text(answer: list) -> str:\n",
    "    \"\"\"Extract only the text from the answers.text column \n",
    "\n",
    "    Args:\n",
    "        answer: the answer.\n",
    "    \"\"\"\n",
    "    if answer:\n",
    "        return answer[0]\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "train_dataframe = pd.json_normalize(data['train'])\n",
    "train_dataframe[\"answers.text\"] = train_dataframe[\"answers.text\"].apply(get_text)\n",
    "\n",
    "val_dataframe = pd.json_normalize(data['validation'])\n",
    "val_dataframe[\"answers.text\"] = val_dataframe[\"answers.text\"].apply(get_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f8b5adf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answers.text</th>\n",
       "      <th>answers.answer_start</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56be85543aeaaa14008c9063</td>\n",
       "      <td>Beyoncé</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "      <td>When did Beyonce start becoming popular?</td>\n",
       "      <td>in the late 1990s</td>\n",
       "      <td>[269]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>56be85543aeaaa14008c9065</td>\n",
       "      <td>Beyoncé</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "      <td>What areas did Beyonce compete in when she was...</td>\n",
       "      <td>singing and dancing</td>\n",
       "      <td>[207]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>56be85543aeaaa14008c9066</td>\n",
       "      <td>Beyoncé</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "      <td>When did Beyonce leave Destiny's Child and bec...</td>\n",
       "      <td>2003</td>\n",
       "      <td>[526]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56bf6b0f3aeaaa14008c9601</td>\n",
       "      <td>Beyoncé</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "      <td>In what city and state did Beyonce  grow up?</td>\n",
       "      <td>Houston, Texas</td>\n",
       "      <td>[166]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>56bf6b0f3aeaaa14008c9602</td>\n",
       "      <td>Beyoncé</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "      <td>In which decade did Beyonce become famous?</td>\n",
       "      <td>late 1990s</td>\n",
       "      <td>[276]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         id    title  \\\n",
       "0  56be85543aeaaa14008c9063  Beyoncé   \n",
       "1  56be85543aeaaa14008c9065  Beyoncé   \n",
       "2  56be85543aeaaa14008c9066  Beyoncé   \n",
       "3  56bf6b0f3aeaaa14008c9601  Beyoncé   \n",
       "4  56bf6b0f3aeaaa14008c9602  Beyoncé   \n",
       "\n",
       "                                             context  \\\n",
       "0  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n",
       "1  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n",
       "2  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n",
       "3  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n",
       "4  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n",
       "\n",
       "                                            question         answers.text  \\\n",
       "0           When did Beyonce start becoming popular?    in the late 1990s   \n",
       "1  What areas did Beyonce compete in when she was...  singing and dancing   \n",
       "2  When did Beyonce leave Destiny's Child and bec...                 2003   \n",
       "3      In what city and state did Beyonce  grow up?        Houston, Texas   \n",
       "4         In which decade did Beyonce become famous?           late 1990s   \n",
       "\n",
       "  answers.answer_start  \n",
       "0                [269]  \n",
       "1                [207]  \n",
       "2                [526]  \n",
       "3                [166]  \n",
       "4                [276]  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "42f32e3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answers.text</th>\n",
       "      <th>answers.answer_start</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56ddde6b9a695914005b9628</td>\n",
       "      <td>Normans</td>\n",
       "      <td>The Normans (Norman: Nourmands; French: Norman...</td>\n",
       "      <td>In what country is Normandy located?</td>\n",
       "      <td>France</td>\n",
       "      <td>[159, 159, 159, 159]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>56ddde6b9a695914005b9629</td>\n",
       "      <td>Normans</td>\n",
       "      <td>The Normans (Norman: Nourmands; French: Norman...</td>\n",
       "      <td>When were the Normans in Normandy?</td>\n",
       "      <td>10th and 11th centuries</td>\n",
       "      <td>[94, 87, 94, 94]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>56ddde6b9a695914005b962a</td>\n",
       "      <td>Normans</td>\n",
       "      <td>The Normans (Norman: Nourmands; French: Norman...</td>\n",
       "      <td>From which countries did the Norse originate?</td>\n",
       "      <td>Denmark, Iceland and Norway</td>\n",
       "      <td>[256, 256, 256, 256]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56ddde6b9a695914005b962b</td>\n",
       "      <td>Normans</td>\n",
       "      <td>The Normans (Norman: Nourmands; French: Norman...</td>\n",
       "      <td>Who was the Norse leader?</td>\n",
       "      <td>Rollo</td>\n",
       "      <td>[308, 308, 308, 308]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>56ddde6b9a695914005b962c</td>\n",
       "      <td>Normans</td>\n",
       "      <td>The Normans (Norman: Nourmands; French: Norman...</td>\n",
       "      <td>What century did the Normans first gain their ...</td>\n",
       "      <td>10th century</td>\n",
       "      <td>[671, 649, 671, 671]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         id    title  \\\n",
       "0  56ddde6b9a695914005b9628  Normans   \n",
       "1  56ddde6b9a695914005b9629  Normans   \n",
       "2  56ddde6b9a695914005b962a  Normans   \n",
       "3  56ddde6b9a695914005b962b  Normans   \n",
       "4  56ddde6b9a695914005b962c  Normans   \n",
       "\n",
       "                                             context  \\\n",
       "0  The Normans (Norman: Nourmands; French: Norman...   \n",
       "1  The Normans (Norman: Nourmands; French: Norman...   \n",
       "2  The Normans (Norman: Nourmands; French: Norman...   \n",
       "3  The Normans (Norman: Nourmands; French: Norman...   \n",
       "4  The Normans (Norman: Nourmands; French: Norman...   \n",
       "\n",
       "                                            question  \\\n",
       "0               In what country is Normandy located?   \n",
       "1                 When were the Normans in Normandy?   \n",
       "2      From which countries did the Norse originate?   \n",
       "3                          Who was the Norse leader?   \n",
       "4  What century did the Normans first gain their ...   \n",
       "\n",
       "                  answers.text  answers.answer_start  \n",
       "0                       France  [159, 159, 159, 159]  \n",
       "1      10th and 11th centuries      [94, 87, 94, 94]  \n",
       "2  Denmark, Iceland and Norway  [256, 256, 256, 256]  \n",
       "3                        Rollo  [308, 308, 308, 308]  \n",
       "4                 10th century  [671, 649, 671, 671]  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1d398cf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    130319.000000\n",
       "mean         58.507739\n",
       "std          73.757111\n",
       "min           1.000000\n",
       "25%          44.000000\n",
       "50%          55.000000\n",
       "75%          69.000000\n",
       "max       25651.000000\n",
       "Name: question, dtype: float64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataframe['question'].apply(len).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "574121fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    130135.000000\n",
       "mean        754.566673\n",
       "std         307.624914\n",
       "min         151.000000\n",
       "25%         561.000000\n",
       "50%         692.000000\n",
       "75%         891.000000\n",
       "max        3706.000000\n",
       "Name: context, dtype: float64"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataframe['context'].apply(len).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ff0b8437",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    130135.000000\n",
       "mean         13.412472\n",
       "std          20.007467\n",
       "min           0.000000\n",
       "25%           0.000000\n",
       "50%           7.000000\n",
       "75%          17.000000\n",
       "max         239.000000\n",
       "Name: answers.text, dtype: float64"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataframe['answers.text'].apply(len).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7b26af",
   "metadata": {},
   "source": [
    "- remove row which have question length more than 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d14707a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataframe = train_dataframe[train_dataframe['question'].apply(len) <= 150]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39bcaac6",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e2047090",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:title={'center':'Context length histogram'}, ylabel='Frequency'>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "figsize = (10,6)\n",
    "train_dataframe['context'].apply(len).plot.hist(title=\"Context length histogram\", \n",
    "                                                bins=20, \n",
    "                                                figsize=figsize, \n",
    "                                                grid=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "804fc935",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:title={'center':'Question length histogram'}, ylabel='Frequency'>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataframe['question'].apply(len).plot.hist(title=\"Question length histogram\", \n",
    "                                                 bins=20, \n",
    "                                                 figsize=figsize, \n",
    "                                                 grid=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c42812ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:title={'center':'Answer length histogram'}, ylabel='Frequency'>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataframe['answers.text'].apply(len).plot.hist(title=\"Answer length histogram\", \n",
    "                                                     bins=100, \n",
    "                                                     figsize=figsize, \n",
    "                                                     grid=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9bb4d094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TITLE: \n",
      "Cotton\n",
      "========================================================================================================================\n",
      "CONTEXT: \n",
      "The era of manufactured fibers began with the development of rayon in France in the 1890s. Rayon is derived from\n",
      "a natural cellulose and cannot be considered synthetic, but requires extensive processing in a manufacturing process, and led the less\n",
      "expensive replacement of more naturally derived materials. A succession of new synthetic fibers were introduced by the chemicals industry in\n",
      "the following decades. Acetate in fiber form was developed in 1924. Nylon, the first fiber synthesized entirely from petrochemicals, was\n",
      "introduced as a sewing thread by DuPont in 1936, followed by DuPont's acrylic in 1944. Some garments were created from\n",
      "fabrics based on these fibers, such as women's hosiery from nylon, but it was not until the introduction of polyester\n",
      "into the fiber marketplace in the early 1950s that the market for cotton came under threat. The rapid uptake of\n",
      "polyester garments in the 1960s caused economic hardship in cotton-exporting economies, especially in Central American countries, such as Nicaragua, where\n",
      "cotton production had boomed tenfold between 1950 and 1965 with the advent of cheap chemical pesticides. Cotton production recovered in\n",
      "the 1970s, but crashed to pre-1960 levels in the early 1990s.\n",
      "========================================================================================================================\n",
      "QUESTION:                                                                ANSWER:                                                                 \n",
      "What was the first manufactured fiber?                                   rayon                                                                   \n",
      "When was rayon first made in France?                                     1890s                                                                   \n",
      "What type of industry produced a growing chain of synthetic fibers?      chemicals industry                                                      \n",
      "What was manufactured completely from petrochemicals?                    Nylon                                                                   \n",
      "What company produced nylon and acrylic in the 1930s and 1940s?          DuPont                                                                  \n",
      "What was the first polyester fiber?                                                                                                              \n",
      "When was polyester first made in France?                                                                                                         \n",
      "What type of industry produced a growing chain of DuPont?                                                                                        \n",
      "What was manufactured completely from DuPont?                                                                                                    \n",
      "What company produced polyester and nylon in the 1930s and 1940s?                                                                                \n"
     ]
    }
   ],
   "source": [
    "#@title Show Samples { run: \"auto\", display-mode: \"form\" }\n",
    "\n",
    "sample_index =  62194 #@param {type:\"slider\", min:0, max:120000, step:1}\n",
    "\n",
    "def print_squad_sample(df: pd.core.frame.DataFrame, line_length: int=20, separator_length: int=120) -> None:\n",
    "    sample = df.iloc[sample_index]\n",
    "    title = sample.title.replace('_', ' ')\n",
    "    print('TITLE: ')\n",
    "    print(title)\n",
    "    print('='*separator_length)\n",
    "    context = sample.context.split()\n",
    "    print('CONTEXT: ')\n",
    "    lines = [' '.join(context[idx:idx+line_length]) for idx in range(0, len(context), line_length)]\n",
    "    for l in lines:\n",
    "        print(l)\n",
    "    print('='*separator_length)\n",
    "    questions = df[df.context.values==sample.context]\n",
    "    max_len = len(max(questions.question, key=len)) + 5\n",
    "    print(\"{: <{max_len}} {: <{max_len}}\".format('QUESTION:','ANSWER:', max_len=max_len))\n",
    "    for idx, row in questions.iterrows():\n",
    "        question = row.question\n",
    "        answer = row['answers.text']\n",
    "        print(\"{: <{max_len}} {: <{max_len}}\".format(question,answer, max_len=max_len))\n",
    "\n",
    "print_squad_sample(train_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015b9673",
   "metadata": {},
   "source": [
    "- we can see that some question doesn't have a answers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03467ed",
   "metadata": {},
   "source": [
    "# Preprocessing the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "0657c7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"deepset/tinyroberta-squad2\"\n",
    "batch_size = 16\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "549b5d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "assert isinstance(tokenizer, transformers.PreTrainedTokenizerFast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "075ffe82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer padding side\n",
    "pad_on_right = tokenizer.padding_side == \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "3b3d0ecc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [0, 2264, 16, 110, 18, 4716, 766, 116, 2, 2, 2387, 4716, 18, 766, 16, 28856, 1851, 4, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"What is your's pet name?\", \"My pet's name is Sylvain.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "b114a5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 384 # The maximum length of a feature (question and context)\n",
    "doc_stride = 128 # The authorized overlap between two part of the context when splitting it is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "37aaa10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, example in enumerate(data['train']):\n",
    "    if len(tokenizer(example[\"question\"], example[\"context\"])[\"input_ids\"]) > 384:\n",
    "        break\n",
    "example = data['train'][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "7c9115d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "442"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer(example[\"question\"], example[\"context\"])[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "3c6c0541",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_example = tokenizer(\n",
    "    example[\"question\"],\n",
    "    example[\"context\"],\n",
    "    max_length=max_length,\n",
    "    truncation=\"only_second\",\n",
    "    return_overflowing_tokens=True,\n",
    "    stride=doc_stride\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "f2113059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>Beyonce got married in 2008 to whom?</s></s>On April 4, 2008, Beyoncé married Jay Z. She publicly revealed their marriage in a video montage at the listening party for her third studio album, I Am... Sasha Fierce, in Manhattan's Sony Club on October 22, 2008. I Am... Sasha Fierce was released on November 18, 2008 in the United States. The album formally introduces Beyoncé's alter ego Sasha Fierce, conceived during the making of her 2003 single \"Crazy in Love\", selling 482,000 copies in its first week, debuting atop the Billboard 200, and giving Beyoncé her third consecutive number-one album in the US. The album featured the number-one song \"Single Ladies (Put a Ring on It)\" and the top-five songs \"If I Were a Boy\" and \"Halo\". Achieving the accomplishment of becoming her longest-running Hot 100 single in her career, \"Halo\"'s success in the US helped Beyoncé attain more top-ten singles on the list than any other woman during the 2000s. It also included the successful \"Sweet Dreams\", and singles \"Diva\", \"Ego\", \"Broken-Hearted Girl\" and \"Video Phone\". The music video for \"Single Ladies\" has been parodied and imitated around the world, spawning the \"first major dance craze\" of the Internet age according to the Toronto Star. The video has won several awards, including Best Video at the 2009 MTV Europe Music Awards, the 2009 Scottish MOBO Awards, and the 2009 BET Awards. At the 2009 MTV Video Music Awards, the video was nominated for nine awards, ultimately winning three including Video of the Year. Its failure to win the Best Female Video category, which went to American country pop singer Taylor Swift's \"You Belong with Me\", led to Kanye West interrupting</s> \n",
      "\n",
      "<s>Beyonce got married in 2008 to whom?</s></s>\". The music video for \"Single Ladies\" has been parodied and imitated around the world, spawning the \"first major dance craze\" of the Internet age according to the Toronto Star. The video has won several awards, including Best Video at the 2009 MTV Europe Music Awards, the 2009 Scottish MOBO Awards, and the 2009 BET Awards. At the 2009 MTV Video Music Awards, the video was nominated for nine awards, ultimately winning three including Video of the Year. Its failure to win the Best Female Video category, which went to American country pop singer Taylor Swift's \"You Belong with Me\", led to Kanye West interrupting the ceremony and Beyoncé improvising a re-presentation of Swift's award during her own acceptance speech. In March 2009, Beyoncé embarked on the I Am... World Tour, her second headlining worldwide concert tour, consisting of 108 shows, grossing $119.5 million.</s> \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for x in tokenized_example[\"input_ids\"]:\n",
    "    print(tokenizer.decode(x), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "ebeb4709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0), (0, 3), (3, 7), (8, 11), (12, 19), (20, 22), (23, 27), (28, 30), (31, 35), (35, 36), (0, 0), (0, 0), (0, 2), (3, 8), (9, 10), (10, 11), (12, 16), (16, 17), (18, 23), (23, 25), (26, 33), (34, 37), (38, 39), (39, 40), (41, 44), (45, 53), (54, 62), (63, 68), (69, 77), (78, 80), (81, 82), (83, 88), (89, 93), (93, 96), (97, 99), (100, 103), (104, 113), (114, 119), (120, 123), (124, 127), (128, 133), (134, 140), (141, 146), (146, 147), (148, 149), (150, 152), (152, 155), (156, 161), (162, 163), (163, 168), (168, 169), (170, 172), (173, 182), (182, 184), (185, 189), (190, 194), (195, 197), (198, 205), (206, 208), (208, 209), (210, 214), (214, 215), (216, 217), (218, 220), (220, 223), (224, 229), (230, 231), (231, 236), (237, 240), (241, 249), (250, 252), (253, 261), (262, 264), (264, 265), (266, 270), (271, 273), (274, 277), (278, 284), (285, 291), (291, 292), (293, 296), (297, 302), (303, 311), (312, 322), (323, 328), (328, 330), (330, 332), (333, 338), (339, 342), (343, 348), (349, 350), (350, 355), (355, 356), (357, 366), (367, 373), (374, 377), (378, 384), (385, 387), (388, 391), (392, 396)]\n"
     ]
    }
   ],
   "source": [
    "tokenized_example = tokenizer(\n",
    "    example[\"question\"],\n",
    "    example[\"context\"],\n",
    "    max_length=max_length,\n",
    "    truncation=\"only_second\",\n",
    "    return_overflowing_tokens=True,\n",
    "    return_offsets_mapping=True,\n",
    "    stride=doc_stride\n",
    ")\n",
    "print(tokenized_example[\"offset_mapping\"][0][:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "7371dbed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bey Bey\n"
     ]
    }
   ],
   "source": [
    "first_token_id = tokenized_example[\"input_ids\"][0][1]\n",
    "offsets = tokenized_example[\"offset_mapping\"][0][1]\n",
    "print(tokenizer.convert_ids_to_tokens([first_token_id])[0], example[\"question\"][offsets[0]:offsets[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "2b810df4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, 0, 0, 0, 0, 0, 0, 0, 0, 0, None, None, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, None]\n"
     ]
    }
   ],
   "source": [
    "sequence_ids = tokenized_example.sequence_ids()\n",
    "print(sequence_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "b9681923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21 22\n"
     ]
    }
   ],
   "source": [
    "answers = example[\"answers\"]\n",
    "start_char = answers[\"answer_start\"][0]\n",
    "end_char = start_char + len(answers[\"text\"][0])\n",
    "\n",
    "# Start token index of the current span in the text.\n",
    "token_start_index = 0\n",
    "while sequence_ids[token_start_index] != 1:\n",
    "    token_start_index += 1\n",
    "\n",
    "# End token index of the current span in the text.\n",
    "token_end_index = len(tokenized_example[\"input_ids\"][0]) - 1\n",
    "while sequence_ids[token_end_index] != 1:\n",
    "    token_end_index -= 1\n",
    "\n",
    "# Detect if the answer is out of the span (in which case this feature is labeled with the CLS index).\n",
    "offsets = tokenized_example[\"offset_mapping\"][0]\n",
    "if (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
    "    # Move the token_start_index and token_end_index to the two ends of the answer.\n",
    "    # Note: we could go after the last offset if the answer is the last word (edge case).\n",
    "    while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
    "        token_start_index += 1\n",
    "    start_position = token_start_index - 1\n",
    "    while offsets[token_end_index][1] >= end_char:\n",
    "        token_end_index -= 1\n",
    "    end_position = token_end_index + 1\n",
    "    print(start_position, end_position)\n",
    "else:\n",
    "    print(\"The answer is not in this feature.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "4e0cf597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Jay Z\n",
      "Jay Z\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(tokenized_example[\"input_ids\"][0][start_position: end_position+1]))\n",
    "print(answers[\"text\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9929a4a1",
   "metadata": {},
   "source": [
    "Now let's put everything together in one function we will apply to our training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "f00ddded",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_train_features(examples):\n",
    "    # Some of the questions have lots of whitespace on the left, which is not useful and will make the\n",
    "    # truncation of the context fail (the tokenized question will take a lots of space). So we remove that\n",
    "    # left whitespace\n",
    "    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n",
    "\n",
    "    # Tokenize our examples with truncation and padding, but keep the overflows using a stride. This results\n",
    "    # in one example possible giving several features when a context is long, each of those features having a\n",
    "    # context that overlaps a bit the context of the previous feature.\n",
    "    tokenized_examples = tokenizer(\n",
    "        examples[\"question\" if pad_on_right else \"context\"],\n",
    "        examples[\"context\" if pad_on_right else \"question\"],\n",
    "        truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
    "        max_length=max_length,\n",
    "        stride=doc_stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    # Since one example might give us several features if it has a long context, we need a map from a feature to\n",
    "    # its corresponding example. This key gives us just that.\n",
    "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "    # The offset mappings will give us a map from token to character position in the original context. This will\n",
    "    # help us compute the start_positions and end_positions.\n",
    "    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n",
    "\n",
    "    # Let's label those examples!\n",
    "    tokenized_examples[\"start_positions\"] = []\n",
    "    tokenized_examples[\"end_positions\"] = []\n",
    "\n",
    "    for i, offsets in enumerate(offset_mapping):\n",
    "        # We will label impossible answers with the index of the CLS token.\n",
    "        input_ids = tokenized_examples[\"input_ids\"][i]\n",
    "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
    "\n",
    "        # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n",
    "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "\n",
    "        # One example can give several spans, this is the index of the example containing this span of text.\n",
    "        sample_index = sample_mapping[i]\n",
    "        answers = examples[\"answers\"][sample_index]\n",
    "        # If no answers are given, set the cls_index as answer.\n",
    "        if len(answers[\"answer_start\"]) == 0:\n",
    "            tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "            tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "        else:\n",
    "            # Start/end character index of the answer in the text.\n",
    "            start_char = answers[\"answer_start\"][0]\n",
    "            end_char = start_char + len(answers[\"text\"][0])\n",
    "\n",
    "            # Start token index of the current span in the text.\n",
    "            token_start_index = 0\n",
    "            while sequence_ids[token_start_index] != (1 if pad_on_right else 0):\n",
    "                token_start_index += 1\n",
    "\n",
    "            # End token index of the current span in the text.\n",
    "            token_end_index = len(input_ids) - 1\n",
    "            while sequence_ids[token_end_index] != (1 if pad_on_right else 0):\n",
    "                token_end_index -= 1\n",
    "\n",
    "            # Detect if the answer is out of the span (in which case this feature is labeled with the CLS index).\n",
    "            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
    "                tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "                tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "            else:\n",
    "                # Otherwise move the token_start_index and token_end_index to the two ends of the answer.\n",
    "                # Note: we could go after the last offset if the answer is the last word (edge case).\n",
    "                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
    "                    token_start_index += 1\n",
    "                tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n",
    "                while offsets[token_end_index][1] >= end_char:\n",
    "                    token_end_index -= 1\n",
    "                tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n",
    "\n",
    "    return tokenized_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "9ed75fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = prepare_train_features(data['train'][:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "f9dd8145",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/130319 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/11873 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets = data.map(prepare_train_features, \n",
    "                              batched=True, \n",
    "                              remove_columns=data[\"train\"].column_names,\n",
    "                              load_from_cache_file=False) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f412effc",
   "metadata": {},
   "source": [
    "# Fine Tune Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "7af58904",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "2c34ff91",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "args = TrainingArguments(\n",
    "    f\"{model_name}-finetuned-squadv2\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    push_to_hub=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "203db395",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import default_data_collator\n",
    "\n",
    "data_collator = default_data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "51ac5655",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b0ad18",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37882310",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"my-tinyroberta-squad2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a061e8",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "94b8242d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['loss', 'start_logits', 'end_logits'])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for batch in trainer.get_eval_dataloader():\n",
    "    break\n",
    "    \n",
    "batch = {k: v.to(trainer.args.device) for k, v in batch.items()}\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = trainer.model(**batch)\n",
    "output.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "91dbabec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 384]), torch.Size([16, 384]))"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.start_logits.shape, output.end_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "cf8359a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 53,  39,  77,  85, 164,   0,   0,   0,   0, 209, 124,  43,   0,   0,\n",
       "           0, 123]),\n",
       " tensor([ 53,  44,  81,  86, 165,   0,   0,   0,   0, 212, 125,  43,   0,   0,\n",
       "           0, 124]))"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.start_logits.argmax(dim=-1), output.end_logits.argmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "a25cf1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_best_size = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "a55f2391",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "start_logits = output.start_logits[0].cpu().numpy()\n",
    "end_logits = output.end_logits[0].cpu().numpy()\n",
    "# Gather the indices the best start/end logits:\n",
    "start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "valid_answers = []\n",
    "for start_index in start_indexes:\n",
    "    for end_index in end_indexes:\n",
    "        if start_index <= end_index: # We need to refine that test to check the answer is inside the context\n",
    "            valid_answers.append(\n",
    "                {\n",
    "                    \"score\": start_logits[start_index] + end_logits[end_index],\n",
    "                    \"text\": \"\" # We need to find a way to get back the original substring corresponding to the answer in the context\n",
    "                }\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "6f3abb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_validation_features(examples):\n",
    "    # Some of the questions have lots of whitespace on the left, which is not useful and will make the\n",
    "    # truncation of the context fail (the tokenized question will take a lots of space). So we remove that\n",
    "    # left whitespace\n",
    "    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n",
    "\n",
    "    # Tokenize our examples with truncation and maybe padding, but keep the overflows using a stride. This results\n",
    "    # in one example possible giving several features when a context is long, each of those features having a\n",
    "    # context that overlaps a bit the context of the previous feature.\n",
    "    tokenized_examples = tokenizer(\n",
    "        examples[\"question\" if pad_on_right else \"context\"],\n",
    "        examples[\"context\" if pad_on_right else \"question\"],\n",
    "        truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
    "        max_length=max_length,\n",
    "        stride=doc_stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    # Since one example might give us several features if it has a long context, we need a map from a feature to\n",
    "    # its corresponding example. This key gives us just that.\n",
    "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "\n",
    "    # We keep the example_id that gave us this feature and we will store the offset mappings.\n",
    "    tokenized_examples[\"example_id\"] = []\n",
    "\n",
    "    for i in range(len(tokenized_examples[\"input_ids\"])):\n",
    "        # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n",
    "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "        context_index = 1 if pad_on_right else 0\n",
    "\n",
    "        # One example can give several spans, this is the index of the example containing this span of text.\n",
    "        sample_index = sample_mapping[i]\n",
    "        tokenized_examples[\"example_id\"].append(examples[\"id\"][sample_index])\n",
    "\n",
    "        # Set to None the offset_mapping that are not part of the context so it's easy to determine if a token\n",
    "        # position is part of the context or not.\n",
    "        tokenized_examples[\"offset_mapping\"][i] = [\n",
    "            (o if sequence_ids[k] == context_index else None)\n",
    "            for k, o in enumerate(tokenized_examples[\"offset_mapping\"][i])\n",
    "        ]\n",
    "\n",
    "    return tokenized_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "f76edca9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/11873 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "validation_features = data[\"validation\"].map(\n",
    "    prepare_validation_features,\n",
    "    batched=True,\n",
    "    remove_columns=data[\"validation\"].column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "5277577d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping. If example_id, offset_mapping are not expected by `RobertaForQuestionAnswering.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 12165\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='43' max='761' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 43/761 05:21 < 1:31:30, 0.13 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [119], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m raw_predictions \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalidation_features\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/NLP-env/lib/python3.9/site-packages/transformers/trainer.py:2870\u001b[0m, in \u001b[0;36mTrainer.predict\u001b[0;34m(self, test_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   2867\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m   2869\u001b[0m eval_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_loop\n\u001b[0;32m-> 2870\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2871\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPrediction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\n\u001b[1;32m   2872\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2873\u001b[0m total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size\n\u001b[1;32m   2874\u001b[0m output\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mupdate(\n\u001b[1;32m   2875\u001b[0m     speed_metrics(\n\u001b[1;32m   2876\u001b[0m         metric_key_prefix,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2880\u001b[0m     )\n\u001b[1;32m   2881\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/NLP-env/lib/python3.9/site-packages/transformers/trainer.py:2974\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   2971\u001b[0m         batch_size \u001b[38;5;241m=\u001b[39m observed_batch_size\n\u001b[1;32m   2973\u001b[0m \u001b[38;5;66;03m# Prediction step\u001b[39;00m\n\u001b[0;32m-> 2974\u001b[0m loss, logits, labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprediction_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2975\u001b[0m inputs_decode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_input(inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39minclude_inputs_for_metrics \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2977\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_torch_tpu_available():\n",
      "File \u001b[0;32m/opt/anaconda3/envs/NLP-env/lib/python3.9/site-packages/transformers/trainer.py:3227\u001b[0m, in \u001b[0;36mTrainer.prediction_step\u001b[0;34m(self, model, inputs, prediction_loss_only, ignore_keys)\u001b[0m\n\u001b[1;32m   3225\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3226\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 3227\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3228\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m   3229\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m outputs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ignore_keys)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/NLP-env/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/anaconda3/envs/NLP-env/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py:1515\u001b[0m, in \u001b[0;36mRobertaForQuestionAnswering.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, start_positions, end_positions, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1503\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1504\u001b[0m \u001b[38;5;124;03mstart_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1505\u001b[0m \u001b[38;5;124;03m    Labels for position (index) of the start of the labelled span for computing the token classification loss.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1511\u001b[0m \u001b[38;5;124;03m    are not taken into account for computing the loss.\u001b[39;00m\n\u001b[1;32m   1512\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1515\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroberta\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1516\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1517\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1518\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1519\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1520\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1521\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1523\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1524\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1525\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1527\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1529\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqa_outputs(sequence_output)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/NLP-env/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/anaconda3/envs/NLP-env/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py:846\u001b[0m, in \u001b[0;36mRobertaModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    837\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m    839\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[1;32m    840\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m    841\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    844\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[1;32m    845\u001b[0m )\n\u001b[0;32m--> 846\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    847\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    848\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    849\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    850\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    851\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    852\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    853\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    854\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    855\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    856\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    858\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    859\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/NLP-env/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/anaconda3/envs/NLP-env/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py:520\u001b[0m, in \u001b[0;36mRobertaEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    511\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[1;32m    512\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m    513\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    517\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    518\u001b[0m     )\n\u001b[1;32m    519\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 520\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    521\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    525\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    530\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/NLP-env/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/anaconda3/envs/NLP-env/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py:405\u001b[0m, in \u001b[0;36mRobertaLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    393\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    394\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    395\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    402\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m    403\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    404\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 405\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    412\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    414\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/NLP-env/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/anaconda3/envs/NLP-env/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py:332\u001b[0m, in \u001b[0;36mRobertaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    324\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    330\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    331\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m--> 332\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    341\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[1;32m    342\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/NLP-env/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/anaconda3/envs/NLP-env/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py:218\u001b[0m, in \u001b[0;36mRobertaSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    216\u001b[0m     value_layer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([past_key_value[\u001b[38;5;241m1\u001b[39m], value_layer], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 218\u001b[0m     key_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranspose_for_scores(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    219\u001b[0m     value_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranspose_for_scores(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue(hidden_states))\n\u001b[1;32m    221\u001b[0m query_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranspose_for_scores(mixed_query_layer)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/NLP-env/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/anaconda3/envs/NLP-env/lib/python3.9/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "raw_predictions = trainer.predict(validation_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c379258",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_features.set_format(type=validation_features.format[\"type\"], \n",
    "                               columns=list(validation_features.features.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99cdf6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_answer_length = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f704bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_logits = output.start_logits[0].cpu().numpy()\n",
    "end_logits = output.end_logits[0].cpu().numpy()\n",
    "offset_mapping = validation_features[0][\"offset_mapping\"]\n",
    "# The first feature comes from the first example. For the more general case, we will need to be match the example_id to\n",
    "# an example index\n",
    "context = datasets[\"validation\"][0][\"context\"]\n",
    "\n",
    "# Gather the indices the best start/end logits:\n",
    "start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "valid_answers = []\n",
    "for start_index in start_indexes:\n",
    "    for end_index in end_indexes:\n",
    "        # Don't consider out-of-scope answers, either because the indices are out of bounds or correspond\n",
    "        # to part of the input_ids that are not in the context.\n",
    "        if (\n",
    "            start_index >= len(offset_mapping)\n",
    "            or end_index >= len(offset_mapping)\n",
    "            or offset_mapping[start_index] is None\n",
    "            or offset_mapping[end_index] is None\n",
    "        ):\n",
    "            continue\n",
    "        # Don't consider answers with a length that is either < 0 or > max_answer_length.\n",
    "        if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n",
    "            continue\n",
    "        if start_index <= end_index: # We need to refine that test to check the answer is inside the context\n",
    "            start_char = offset_mapping[start_index][0]\n",
    "            end_char = offset_mapping[end_index][1]\n",
    "            valid_answers.append(\n",
    "                {\n",
    "                    \"score\": start_logits[start_index] + end_logits[end_index],\n",
    "                    \"text\": context[start_char: end_char]\n",
    "                }\n",
    "            )\n",
    "\n",
    "valid_answers = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[:n_best_size]\n",
    "valid_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa76f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets[\"validation\"][0][\"answers\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8685c98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "examples = datasets[\"validation\"]\n",
    "features = validation_features\n",
    "\n",
    "example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n",
    "features_per_example = collections.defaultdict(list)\n",
    "for i, feature in enumerate(features):\n",
    "    features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808c1a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "def postprocess_qa_predictions(examples, features, raw_predictions, n_best_size = 20, max_answer_length = 30):\n",
    "    all_start_logits, all_end_logits = raw_predictions\n",
    "    # Build a map example to its corresponding features.\n",
    "    example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n",
    "    features_per_example = collections.defaultdict(list)\n",
    "    for i, feature in enumerate(features):\n",
    "        features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)\n",
    "\n",
    "    # The dictionaries we have to fill.\n",
    "    predictions = collections.OrderedDict()\n",
    "\n",
    "    # Logging.\n",
    "    print(f\"Post-processing {len(examples)} example predictions split into {len(features)} features.\")\n",
    "\n",
    "    # Let's loop over all the examples!\n",
    "    for example_index, example in enumerate(tqdm(examples)):\n",
    "        # Those are the indices of the features associated to the current example.\n",
    "        feature_indices = features_per_example[example_index]\n",
    "\n",
    "        min_null_score = None # Only used if squad_v2 is True.\n",
    "        valid_answers = []\n",
    "        \n",
    "        context = example[\"context\"]\n",
    "        # Looping through all the features associated to the current example.\n",
    "        for feature_index in feature_indices:\n",
    "            # We grab the predictions of the model for this feature.\n",
    "            start_logits = all_start_logits[feature_index]\n",
    "            end_logits = all_end_logits[feature_index]\n",
    "            # This is what will allow us to map some the positions in our logits to span of texts in the original\n",
    "            # context.\n",
    "            offset_mapping = features[feature_index][\"offset_mapping\"]\n",
    "\n",
    "            # Update minimum null prediction.\n",
    "            cls_index = features[feature_index][\"input_ids\"].index(tokenizer.cls_token_id)\n",
    "            feature_null_score = start_logits[cls_index] + end_logits[cls_index]\n",
    "            if min_null_score is None or min_null_score < feature_null_score:\n",
    "                min_null_score = feature_null_score\n",
    "\n",
    "            # Go through all possibilities for the `n_best_size` greater start and end logits.\n",
    "            start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "            end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "            for start_index in start_indexes:\n",
    "                for end_index in end_indexes:\n",
    "                    # Don't consider out-of-scope answers, either because the indices are out of bounds or correspond\n",
    "                    # to part of the input_ids that are not in the context.\n",
    "                    if (\n",
    "                        start_index >= len(offset_mapping)\n",
    "                        or end_index >= len(offset_mapping)\n",
    "                        or offset_mapping[start_index] is None\n",
    "                        or offset_mapping[end_index] is None\n",
    "                    ):\n",
    "                        continue\n",
    "                    # Don't consider answers with a length that is either < 0 or > max_answer_length.\n",
    "                    if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n",
    "                        continue\n",
    "\n",
    "                    start_char = offset_mapping[start_index][0]\n",
    "                    end_char = offset_mapping[end_index][1]\n",
    "                    valid_answers.append(\n",
    "                        {\n",
    "                            \"score\": start_logits[start_index] + end_logits[end_index],\n",
    "                            \"text\": context[start_char: end_char]\n",
    "                        }\n",
    "                    )\n",
    "        \n",
    "        if len(valid_answers) > 0:\n",
    "            best_answer = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[0]\n",
    "        else:\n",
    "            # In the very rare edge case we have not a single non-null prediction, we create a fake prediction to avoid\n",
    "            # failure.\n",
    "            best_answer = {\"text\": \"\", \"score\": 0.0}\n",
    "        \n",
    "        # Let's pick our final answer: the best one or the null answer (only for squad_v2)\n",
    "        if not squad_v2:\n",
    "            predictions[example[\"id\"]] = best_answer[\"text\"]\n",
    "        else:\n",
    "            answer = best_answer[\"text\"] if best_answer[\"score\"] > min_null_score else \"\"\n",
    "            predictions[example[\"id\"]] = answer\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07d929a",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_predictions = postprocess_qa_predictions(datasets[\"validation\"], \n",
    "                                               validation_features, \n",
    "                                               raw_predictions.predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30c2cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = load_metric(\"squad_v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eede4a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_predictions = [{\"id\": k, \"prediction_text\": v, \"no_answer_probability\": 0.0} for k, v in final_predictions.items()]\n",
    "\n",
    "references = [{\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in datasets[\"validation\"]]\n",
    "metric.compute(predictions=formatted_predictions, references=references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0933cdc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.push_to_hub()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-nb",
   "language": "python",
   "name": "nlp-nb"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
